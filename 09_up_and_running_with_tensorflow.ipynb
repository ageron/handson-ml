{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 9 â€“ Up and running with TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercices in chapter 9._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "rnd.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "print(sess.run(f))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session():\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "sess.close()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Cal. housing from http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz to /root/scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float64, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float64, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = theta.eval()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with pure NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+00   6.60969987e-17   5.50808322e-18   6.60969987e-17\n",
      "  -1.06030602e-16  -1.10161664e-17   3.44255201e-18  -1.07958431e-15\n",
      "  -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ..., -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.111111111111\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.75443\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.57278\n",
      "Epoch 300 MSE = 0.558501\n",
      "Epoch 400 MSE = 0.549069\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.537379\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.531243\n",
      "Epoch 900 MSE = 0.529371\n",
      "Best theta:\n",
      "[[  2.06855226e+00]\n",
      " [  7.74078071e-01]\n",
      " [  1.31192386e-01]\n",
      " [ -1.17845096e-01]\n",
      " [  1.64778158e-01]\n",
      " [  7.44080753e-04]\n",
      " [ -3.91945168e-02]\n",
      " [ -8.61356616e-01]\n",
      " [ -8.23479712e-01]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff\n",
    "Same as above except for the `gradients = ...` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.75443\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.57278\n",
      "Epoch 300 MSE = 0.558501\n",
      "Epoch 400 MSE = 0.549069\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.537379\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.531243\n",
      "Epoch 900 MSE = 0.529371\n",
      "Best theta:\n",
      "[[  2.06855249e+00]\n",
      " [  7.74078071e-01]\n",
      " [  1.31192386e-01]\n",
      " [ -1.17845066e-01]\n",
      " [  1.64778143e-01]\n",
      " [  7.44078017e-04]\n",
      " [ -3.91945094e-02]\n",
      " [ -8.61356676e-01]\n",
      " [ -8.23479772e-01]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.75443\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.57278\n",
      "Epoch 300 MSE = 0.558501\n",
      "Epoch 400 MSE = 0.549069\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.537379\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.531243\n",
      "Epoch 900 MSE = 0.529371\n",
      "Best theta:\n",
      "[[  2.06855249e+00]\n",
      " [  7.74078071e-01]\n",
      " [  1.31192386e-01]\n",
      " [ -1.17845066e-01]\n",
      " [  1.64778143e-01]\n",
      " [  7.44078017e-04]\n",
      " [ -3.91945094e-02]\n",
      " [ -8.61356676e-01]\n",
      " [ -8.23479772e-01]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.25)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[  2.06855392e+00]\n",
      " [  7.94067979e-01]\n",
      " [  1.25333667e-01]\n",
      " [ -1.73580602e-01]\n",
      " [  2.18767926e-01]\n",
      " [ -1.64708309e-03]\n",
      " [ -3.91250364e-02]\n",
      " [ -8.85289013e-01]\n",
      " [ -8.50607991e-01]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    ">>> tf.reset_default_graph()\n",
    "\n",
    ">>> A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    ">>> B = A + 5\n",
    ">>> with tf.Session() as sess:\n",
    "...     B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "...     B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "...\n",
    ">>> print(B_val_1)\n",
    ">>> print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.07001591]\n",
      " [ 0.82045609]\n",
      " [ 0.1173173 ]\n",
      " [-0.22739051]\n",
      " [ 0.31134021]\n",
      " [ 0.00353193]\n",
      " [-0.01126994]\n",
      " [-0.91643935]\n",
      " [-0.87950081]]\n"
     ]
    }
   ],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    rnd.seed(epoch * n_batches + batch_index)\n",
    "    indices = rnd.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.75443\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.57278\n",
      "Epoch 300 MSE = 0.558501\n",
      "Epoch 400 MSE = 0.549069\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.537379\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.531243\n",
      "Epoch 900 MSE = 0.529371\n",
      "Best theta:\n",
      "[[  2.06855249e+00]\n",
      " [  7.74078071e-01]\n",
      " [  1.31192386e-01]\n",
      " [ -1.17845066e-01]\n",
      " [  1.64778143e-01]\n",
      " [  7.44078017e-04]\n",
      " [ -3.91945094e-02]\n",
      " [ -8.61356676e-01]\n",
      " [ -8.23479772e-01]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.1784179106&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 9\\n          }\\n        }\\n        tensor_content: &quot;<stripped 743040 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 82560 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 87654321\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mse_grad/Reshape&quot;\\n  input: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape&quot;\\n  input: &quot;gradients/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape_1&quot;\\n  input: &quot;gradients/mse_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/mse_grad/Prod_1&quot;\\n  input: &quot;gradients/mse_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/mse_grad/Prod&quot;\\n  input: &quot;gradients/mse_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/mse_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mse_grad/Tile&quot;\\n  input: &quot;gradients/mse_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square_grad/mul/x&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mse_grad/truediv&quot;\\n  input: &quot;gradients/Square_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Sum&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.00999999977648\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.1784179106&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.07001591]\n",
      " [ 0.82045609]\n",
      " [ 0.1173173 ]\n",
      " [-0.22739051]\n",
      " [ 0.31134021]\n",
      " [ 0.00353193]\n",
      " [-0.01126994]\n",
      " [-0.91643935]\n",
      " [-0.87950081]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.07001591]\n",
      " [ 0.82045609]\n",
      " [ 0.1173173 ]\n",
      " [-0.22739051]\n",
      " [ 0.31134021]\n",
      " [ 0.00353193]\n",
      " [-0.01126994]\n",
      " [-0.91643935]\n",
      " [-0.87950081]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "linear1 = tf.add(tf.matmul(X, w1), b1, name=\"linear1\")\n",
    "linear2 = tf.add(tf.matmul(X, w2), b2, name=\"linear2\")\n",
    "\n",
    "relu1 = tf.maximum(linear1, 0, name=\"relu1\")\n",
    "relu2 = tf.maximum(linear1, 0, name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add_n([relu1, relu2], name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = int(X.get_shape()[1]), 1\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    linear = tf.add(tf.matmul(X, w), b, name=\"linear\")\n",
    "    return tf.maximum(linear, 0, name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "summary_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = int(X.get_shape()[1]), 1\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        linear = tf.add(tf.matmul(X, w), b, name=\"linear\")\n",
    "        return tf.maximum(linear, 0, name=\"max\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a `threshold` variable the classic way, by defining it outside of the `relu()` function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = int(X.get_shape()[1]), 1\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        linear = tf.add(tf.matmul(X, w), b, name=\"linear\")\n",
    "        return tf.maximum(linear, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        linear = tf.add(tf.matmul(X, w), b, name=\"linear\")\n",
    "        return tf.maximum(linear, relu.threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = int(X.get_shape()[1]), 1\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        linear = tf.add(tf.matmul(X, w), b, name=\"linear\")\n",
    "        return tf.maximum(linear, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = int(X.get_shape()[1]), 1\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        linear = tf.add(tf.matmul(X, w), b, name=\"linear\")\n",
    "        return tf.maximum(linear, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "param/x\n",
      "param/x\n",
      "param/x\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"param\"):\n",
    "    x = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    #x = tf.Variable(0., name=\"x\")\n",
    "with tf.variable_scope(\"param\", reuse=True):\n",
    "    y = tf.get_variable(\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    z = tf.get_variable(\"param/x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "\n",
    "print(x is y)\n",
    "print(x.op.name)\n",
    "print(y.op.name)\n",
    "print(z.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do' 'you' 'want' 'some' 'caf\\xc3\\xa9?']\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "text = np.array(\"Do you want some cafÃ©?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server = tf.train.Server.create_local_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(2) + tf.constant(3)\n",
    "with tf.Session(server.target) as sess:\n",
    "    print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grpc://localhost:60347'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self, **variables):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def evaluate(self, **variables):\n",
    "        return variables[self.name]\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self, **variables):\n",
    "        return self.a.evaluate(**variables) + self.b.evaluate(**variables)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self, **variables):\n",
    "        return self.a.evaluate(**variables) * self.b.evaluate(**variables)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate(x=3, y=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "df_dx = Mul(Const(2), Mul(Var(\"x\"), Var(\"y\")))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(Var(\"x\"), Var(\"x\")), Const(1))  # df/dy = xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate(x=3, y=4))\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate(x=3, y=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.0004\n",
      "df/dy(3,4) = 10.0\n"
     ]
    }
   ],
   "source": [
    "def derivative(f, x, y, x_eps, y_eps):\n",
    "    return (f.evaluate(x = x + x_eps, y = y + y_eps) - f.evaluate(x = x, y = y)) / (x_eps + y_eps)\n",
    "\n",
    "df_dx_34 = derivative(f, x=3, y=4, x_eps=0.0001, y_eps=0)\n",
    "df_dy_34 = derivative(f, x=3, y=4, x_eps=0, y_eps=0.0001)\n",
    "print(\"df/dx(3,4) =\", df_dx_34)\n",
    "print(\"df/dy(3,4) =\", df_dy_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2*y + y + 2\n",
    "\n",
    "def derivative(f, x, y, x_eps, y_eps):\n",
    "    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)\n",
    "\n",
    "df_dx = derivative(f, 3, 4, 0.00001, 0)\n",
    "df_dy = derivative(f, 3, 4, 0, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0000399998\n",
      "10.0000000003\n"
     ]
    }
   ],
   "source": [
    "print(df_dx)\n",
    "print(df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self.name==var else Const(0)\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "\n",
    "x = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "df_dx = f.derive(\"x\")  # 2xy\n",
    "df_dy = f.derive(\"y\")  # xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate(x=3, y=4))\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate(x=3, y=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation (autodiff) â€“ forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(3,4) = 42\n",
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self, derive, **variables):\n",
    "        return self.value, 0\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def evaluate(self, derive, **variables):\n",
    "        return variables[self.name], (1 if derive == self.name else 0)\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self, derive, **variables):\n",
    "        a, da = self.a.evaluate(derive, **variables)\n",
    "        b, db = self.b.evaluate(derive, **variables)\n",
    "        return a + b, da + db\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self, derive, **variables):\n",
    "        a, da = self.a.evaluate(derive, **variables)\n",
    "        b, db = self.b.evaluate(derive, **variables)\n",
    "        return a * b, a * db + da * b\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "f34, df_dx_34 = f.evaluate(x=3, y=4, derive=\"x\")\n",
    "f34, df_dy_34 = f.evaluate(x=3, y=4, derive=\"y\")\n",
    "print(\"f(3,4) =\", f34)\n",
    "print(\"df/dx(3,4) =\", df_dx_34)\n",
    "print(\"df/dy(3,4) =\", df_dy_34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.derivative = 0\n",
    "        self.value = value\n",
    "    def evaluate(self, **variables):\n",
    "        return self.value\n",
    "    def backpropagate(self, derivative):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def evaluate(self, **variables):\n",
    "        self.derivative = 0\n",
    "        self.value = variables[self.name]\n",
    "        return self.value\n",
    "    def backpropagate(self, derivative):\n",
    "        self.derivative += derivative\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self, **variables):\n",
    "        self.derivative = 0\n",
    "        self.value = self.a.evaluate(**variables) + self.b.evaluate(**variables)\n",
    "        return self.value\n",
    "    def backpropagate(self, derivative):\n",
    "        self.derivative += derivative\n",
    "        self.a.backpropagate(derivative)\n",
    "        self.b.backpropagate(derivative)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self, **variables):\n",
    "        self.derivative = 0\n",
    "        self.value = self.a.evaluate(**variables) * self.b.evaluate(**variables)\n",
    "        return self.value\n",
    "    def backpropagate(self, derivative):\n",
    "        self.derivative += derivative\n",
    "        self.a.backpropagate(derivative * self.b.value)\n",
    "        self.b.backpropagate(derivative * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(3,4) = 42\n",
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "x = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "f34 = f.evaluate(x=3, y=4)\n",
    "f.backpropagate(1)\n",
    "print(\"f(3,4) =\", f34)\n",
    "print(\"df/dx(3,4) =\", x.derivative)\n",
    "print(\"df/dy(3,4) =\", y.derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"x\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. What are the main benefits of creating a computation graph rather than directly executing the computations? What are the main drawbacks?\n",
    "\n",
    "   Benefits: Automated tools can parallelize the workflow across available\n",
    "   processing units, tools like Tensorboard can visualize the workflow.\n",
    "\n",
    "   Drawbacks: Harder to reason about, actual calculations are less transparent.\n",
    "   (E.g., tfdbg is the best way to debug a broken calculation.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Is the statement `aval=a.eval(session=sess)` equivalent to `aval=sess.run(a)`?\n",
    "\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Is the statement `aval, bval=a.eval(session=sess), b.eval(session=sess)`  equivalent to `aval, bval=sess.run([a, b])`?\n",
    "\n",
    "   No. The first statement results in two executions of the computation graph,\n",
    "   the second only one. For the first statement, the first evaluation only\n",
    "   executes nodes necessary to compute `a`, the second only those needed for\n",
    "   `b`. If execution of the graph for `a` changes parameter values, the results\n",
    "   for `b` could be different. If nodes necessary for `b` change the calculation\n",
    "   for `a`, the value of `a` could be different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Can you run two graphs in the same session?\n",
    "\n",
    "   [No](https://www.tensorflow.org/api_docs/python/tf/Session#__init__): \"If you are using more than one graph (created with tf.Graph() in the\n",
    "   same process, you will have to use different sessions for each graph, but\n",
    "   each graph can be used in multiple sessions.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. If you create a graph g containing a variable w, then start two threads an open a session in each thread, both using the same graph g, will each session have its own copy of the variable w or will it be shared?\n",
    "\n",
    "The value is shared. Test code, where the `report_thread` checks that it sees the modification made by the `mod_thread`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "modification thread initial value: [0][1]\n",
      "\n",
      "[1]\n",
      "following modification: [1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "import threading, time\n",
    "\n",
    "theta = tf.Variable(0)\n",
    "mod_op = theta.assign(1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "def mod_thread():\n",
    "    time.sleep(1)\n",
    "    print('modification thread initial value:', sess.run([theta]))\n",
    "    print(sess.run([mod_op]))\n",
    "    print('following modification:', sess.run([theta]))\n",
    "\n",
    "def report_thread():\n",
    "    for i in range(3):\n",
    "        time.sleep(0.5)\n",
    "        print(sess.run([theta]))\n",
    "\n",
    "threads = [threading.Thread(target=mod_thread),\n",
    "           threading.Thread(target=report_thread)]\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  When is a variable initialized? When is it destroyed?\n",
    "Initialized when you run the relevant initializer op in a session. Presumably\n",
    "destroyed when the containing graph is destroyed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is the difference between a placeholder and a variable?\n",
    "A variable contains a value. A placeholder receives a value at runtime, e.g.\n",
    "via the feed_dict argument to Session.run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What happens when you run the graph to evaluate an operation that depends on a placeholder but you donâ€™t feed its value? What happens if the operation does not depend on the placeholder?\n",
    "\n",
    "For the first question, you get an **`InvalidArgumentError: You must feed a value for placeholder tensor`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must feed a value for placeholder tensor 'Placeholder' with dtype float\n",
      "\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "sess = tf.Session()\n",
    "try:\n",
    "    sess.run([x])\n",
    "except tf.errors.InvalidArgumentError, e:\n",
    "    print(e.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second question, the calculation goes through without problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This runs without problems: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'x:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "y = tf.Variable(0, name='y')\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run([init])\n",
    "print('This runs without problems:', sess.run([y]))\n",
    "tf.get_default_graph().get_tensor_by_name(\"x:0\") # Verify that x is in the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. When you run a graph, can you feed the output value of any operation, or just the value of placeholders?\n",
    "\n",
    "Any. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(40, dtype=int32)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(30)\n",
    "op = x + 5\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run([init])\n",
    "print(sess.run([op]))\n",
    "sess.run([op], feed_dict={op: 40})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. How can you set a variable to any value you want (during the execution phase)?\n",
    "\n",
    "    You can use `Variable.assign(x)`, as long as `x` is of a compatible type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. How many times does reverse-mode autodiff need to traverse the graph in order to compute the gradients of the cost function with regards to 10 variables? What about forward-mode autodiff? And symbolic differentiation? \n",
    "    \n",
    "Reverse-mode: Twice; once forward and once backward for the single output variable.\n",
    "\n",
    "Forward mode: Roughly ten times; once for each input variable, but only the parts of the graph to which that variable pertains.\n",
    "\n",
    "Symbolic: Twice; once for the symbolic calculation of the derivatives, then once for the numeric calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow. \n",
    "Train it and evaluate it on the moons dataset (introduced in\n",
    "Chapter 5). Try adding all the bells and whistles:\n",
    "\n",
    "* Define the graph within a logistic_regression() function that can be\n",
    "  reused easily.\n",
    "* Save checkpoints using a Saver at regular intervals during training, and\n",
    "  save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the\n",
    "  mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the machinery for this is in the \"Mini-Batch Gradient Descent\" section above. Copying from there, and [this example](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, tempfile, os, glob, collections, re\n",
    "\n",
    "tempdir = tempfile.mkdtemp()\n",
    "logdir = os.path.join(tempdir, 'lr', 'graph')\n",
    "chkpath = os.path.join(tempdir, 'chkpt-%s' % time.time())\n",
    "\n",
    "LRTensors = collections.namedtuple(\n",
    "    'LRTensors', 'init W b pred cost training_op summaries writer saver')\n",
    "\n",
    "Parameters = collections.namedtuple('Parameters', 'W b')\n",
    "\n",
    "def logistic_regression(inputs, outputs, learning_rate=0.01):\n",
    "    \"\"\"Generate a TF graph for training a logistic regression model.\n",
    "    \n",
    "    Training occurs by optimizing the softmax probability of the observed events\n",
    "    in the training data, assuming the observations are IID.\n",
    "    \n",
    "    `inputs` and `outputs` are placeholder tensors for batched training data.\n",
    "    `outputs` values should be one-hot encoded over the possible events\n",
    "    \n",
    "    Returns tuple containing tensors for initializer, weights, bias, the predicted \n",
    "    logistic probabilities for the events, the cost, and the training, summary and \n",
    "    saving operations.\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope('lr'):\n",
    "        n, m = int(inputs.shape[1]), int(outputs.shape[1])\n",
    "        W = tf.Variable(tf.random_uniform([n, m], -1.0, 1.0, seed=42), \n",
    "                        name=\"parameters/weights\")\n",
    "        b = tf.Variable([0]*n, dtype=tf.float32, name='parameters/bias')\n",
    "        parameters = Parameters(W, b)\n",
    "        pred = tf.nn.softmax(tf.matmul(inputs, W) + b, name='outputs/probs')\n",
    "        # Extract the predicted log probability of the observed event for each \n",
    "        # training case. Each row of outputs contains '1' in observed column\n",
    "        # and '0' in all other columns, so summing the rows of \n",
    "        # outputs*tf.log(pred) gives a single nonzero summand for each.\n",
    "        probs = -tf.reduce_sum(outputs*tf.log(pred),\n",
    "                               reduction_indices=1, name='outputs/logprobs')\n",
    "        # The total probability of the observations, under the IID assumption.\n",
    "        cost = tf.reduce_mean(probs, name='outputs/cost')\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                              momentum=0.25)\n",
    "        training_op = optimizer.minimize(cost)\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver(parameters._asdict())\n",
    "        cost_summary = tf.summary.scalar('cost', cost)\n",
    "        summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "        summaries = tf.summary.merge_all()\n",
    "        return LRTensors(init, W, b, pred, cost, training_op, summaries, \n",
    "                         summary_writer, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons as mm\n",
    "position, is_moon = mm(1000000, random_state=42)\n",
    "moon_1hot = np.zeros((position.shape[0], len(set(is_moon))))\n",
    "for rowidx, moonp in enumerate(is_moon):  # Should be some np magic for this.\n",
    "    moon_1hot[rowidx, moonp] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_positions, test_positions, train_moon, test_moon = (\n",
    "    train_test_split(position, moon_1hot, test_size=0.20, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cost: 0.86902 test cost: 0.812872\n",
      "train cost: 0.24521 test cost: 0.243336\n",
      "train cost: 0.24344 test cost: 0.240622\n",
      "train cost: 0.24329 test cost: 0.240232\n",
      "train cost: 0.24327 test cost: 0.240139\n",
      "train cost: 0.24327 test cost: 0.240106\n",
      "train cost: 0.24327 test cost: 0.240097\n",
      "train cost: 0.24327 test cost: 0.240094\n",
      "train cost: 0.24327 test cost: 0.240093\n",
      "train cost: 0.24327 test cost: 0.240093\n",
      "train cost: 0.24327 test cost: 0.240093\n",
      "Best parameters:\n",
      "Parameters(W=array([[ 0.09289816,  1.16656089],\n",
      "       [ 3.69480968, -2.59261751]], dtype=float32), b=array([-0.69547945,  0.69548136], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    rnd.seed(epoch * n_batches + batch_index)\n",
    "    indices = rnd.randint(train_positions.shape[0], size=batch_size)\n",
    "    X_batch = train_positions[indices]\n",
    "    y_batch = train_moon[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, position.shape[1]),\n",
    "                   name='X_batch')\n",
    "y = tf.placeholder(tf.float32, shape=(None, moon_1hot.shape[1]),\n",
    "                   name='Y_batch')\n",
    "\n",
    "tensors = logistic_regression(X, y, learning_rate=0.1)\n",
    "\n",
    "sess.run(tensors.init)\n",
    "\n",
    "# Check whether there's a checkpoint to restore from\n",
    "checkpoints = glob.glob(chkpath + '-*')\n",
    "if checkpoints:  # Restore from last checkpoint\n",
    "    # Remove the suffix from the last checkpoint\n",
    "    last_chk = re.sub(r'\\.[^.]+$', '', max(checkpoints))\n",
    "    tensors.saver.restore(sess, last_chk)\n",
    "\n",
    "epoch = 0\n",
    "best_test_cost = np.inf\n",
    "\n",
    "while True:\n",
    "    for batch_index in range(n_batches):\n",
    "        X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "    _, ccost = sess.run([tensors.training_op, tensors.cost], \n",
    "                        feed_dict={X: X_batch, y: y_batch})\n",
    "    if epoch % 1000 == 0:\n",
    "        [test_cost, test_summaries] = sess.run(\n",
    "            [tensors.cost, tensors.summaries], \n",
    "            feed_dict={X: test_positions, y: test_moon})\n",
    "        print('train cost: %.5f' % ccost, 'test cost:', test_cost)\n",
    "        tensors.saver.save(sess, chkpath, global_step=epoch)\n",
    "        tensors.writer.add_summary(test_summaries, epoch)\n",
    "        tensors.writer.flush()  # Ensure tensorboard can read summary\n",
    "        if test_cost >= best_test_cost:  # Early stopping rule\n",
    "            break\n",
    "        best_test_cost = test_cost\n",
    "    epoch += 1\n",
    "\n",
    "parameters = Parameters(*sess.run([tensors.W, tensors.b]))\n",
    "    \n",
    "print(\"Best parameters:\")\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD/CAYAAAAQaHZxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny5k02YBkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4e85ee050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(*zip(*position[:1000]), c=['rb'[m] for m in is_moon[:1000]])\n",
    "\n",
    "xbounds, ybounds = plt.xlim(), plt.ylim()\n",
    "x, y = [np.arange(*b, step=0.01) for b in [xbounds, ybounds]]\n",
    "xy = [(x0, y0) for y0 in y for x0 in x]\n",
    "[probs] = sess.run([tensors.pred], feed_dict={X: xy})\n",
    "# x, y reversed because first index to probs is rows, second is columns.\n",
    "probs = (probs[:, 0].reshape((len(y), len(x))))\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "plt.contourf(xv, yv, probs, 20, cmap=plt.cm.rainbow, alpha=0.3)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.84185493438&quot;).pbtxt = 'node {\\n  name: &quot;X_batch&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Y_batch&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;lr/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 87654321\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;lr/random_uniform/max&quot;\\n  input: &quot;lr/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;lr/random_uniform/RandomUniform&quot;\\n  input: &quot;lr/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;lr/random_uniform/mul&quot;\\n  input: &quot;lr/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/parameters/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 2\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/parameters/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;lr/parameters/weights&quot;\\n  input: &quot;lr/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/parameters/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/parameters/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/parameters/bias/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/parameters/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/parameters/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;lr/parameters/bias&quot;\\n  input: &quot;lr/parameters/bias/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/parameters/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/parameters/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X_batch&quot;\\n  input: &quot;lr/parameters/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;lr/MatMul&quot;\\n  input: &quot;lr/parameters/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/outputs/probs&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;lr/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Log&quot;\\n  op: &quot;Log&quot;\\n  input: &quot;lr/outputs/probs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Y_batch&quot;\\n  input: &quot;lr/Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/outputs/logprobs/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/outputs/logprobs&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;lr/mul&quot;\\n  input: &quot;lr/outputs/logprobs/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;lr/outputs/logprobs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/outputs/cost&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;lr/Neg&quot;\\n  input: &quot;lr/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;lr/gradients/Shape&quot;\\n  input: &quot;lr/gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;lr/gradients/Fill&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;lr/Neg&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Reshape&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;lr/Neg&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Shape_1&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Shape_2&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Prod_1&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Prod&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/cost_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Tile&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/Neg_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;lr/gradients/lr/outputs/cost_grad/truediv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;lr/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;lr/outputs/logprobs/reduction_indices&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/add&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/range/start&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Size&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Shape_1&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/range&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/mod&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Shape&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/DynamicStitch&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Shape&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;lr/gradients/lr/Neg_grad/Neg&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/logprobs_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Reshape&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Y_batch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;lr/Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Shape&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Tile&quot;\\n  input: &quot;lr/Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/mul&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Sum&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Y_batch&quot;\\n  input: &quot;lr/gradients/lr/outputs/logprobs_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/mul_1&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Sum_1&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^lr/gradients/lr/mul_grad/Reshape&quot;\\n  input: &quot;^lr/gradients/lr/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Reshape&quot;\\n  input: &quot;^lr/gradients/lr/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/gradients/lr/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/Reshape_1&quot;\\n  input: &quot;^lr/gradients/lr/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/gradients/lr/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/Log_grad/Reciprocal&quot;\\n  op: &quot;Reciprocal&quot;\\n  input: &quot;lr/outputs/probs&quot;\\n  input: &quot;^lr/gradients/lr/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/Log_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;lr/gradients/lr/mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;lr/gradients/lr/Log_grad/Reciprocal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/probs_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;lr/gradients/lr/Log_grad/mul&quot;\\n  input: &quot;lr/outputs/probs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/probs_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/probs_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/mul&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/probs_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/probs_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/Sum&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/probs_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;lr/gradients/lr/Log_grad/mul&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/outputs/probs_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/sub&quot;\\n  input: &quot;lr/outputs/probs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;lr/MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Shape&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/mul_1&quot;\\n  input: &quot;lr/gradients/lr/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Sum&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;lr/gradients/lr/outputs/probs_grad/mul_1&quot;\\n  input: &quot;lr/gradients/lr/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Sum_1&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^lr/gradients/lr/add_grad/Reshape&quot;\\n  input: &quot;^lr/gradients/lr/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Reshape&quot;\\n  input: &quot;^lr/gradients/lr/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/gradients/lr/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/gradients/lr/add_grad/Reshape_1&quot;\\n  input: &quot;^lr/gradients/lr/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/gradients/lr/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;lr/gradients/lr/add_grad/tuple/control_dependency&quot;\\n  input: &quot;lr/parameters/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X_batch&quot;\\n  input: &quot;lr/gradients/lr/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^lr/gradients/lr/MatMul_grad/MatMul&quot;\\n  input: &quot;^lr/gradients/lr/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/gradients/lr/MatMul_grad/MatMul&quot;\\n  input: &quot;^lr/gradients/lr/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/gradients/lr/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/gradients/lr/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/gradients/lr/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^lr/gradients/lr/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/gradients/lr/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/weights/Momentum/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n          dim {\\n            size: 2\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/weights/Momentum&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 2\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/weights/Momentum/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;lr/lr/parameters/weights/Momentum&quot;\\n  input: &quot;lr/lr/parameters/weights/Momentum/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/weights/Momentum/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/lr/parameters/weights/Momentum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/bias/Momentum/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/bias/Momentum&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/bias/Momentum/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;lr/lr/parameters/bias/Momentum&quot;\\n  input: &quot;lr/lr/parameters/bias/Momentum/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/lr/parameters/bias/Momentum/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/lr/parameters/bias/Momentum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Momentum/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Momentum/momentum&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.25\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Momentum/update_lr/parameters/weights/ApplyMomentum&quot;\\n  op: &quot;ApplyMomentum&quot;\\n  input: &quot;lr/parameters/weights&quot;\\n  input: &quot;lr/lr/parameters/weights/Momentum&quot;\\n  input: &quot;lr/Momentum/learning_rate&quot;\\n  input: &quot;lr/gradients/lr/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;lr/Momentum/momentum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;use_nesterov&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Momentum/update_lr/parameters/bias/ApplyMomentum&quot;\\n  op: &quot;ApplyMomentum&quot;\\n  input: &quot;lr/parameters/bias&quot;\\n  input: &quot;lr/lr/parameters/bias/Momentum&quot;\\n  input: &quot;lr/Momentum/learning_rate&quot;\\n  input: &quot;lr/gradients/lr/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;lr/Momentum/momentum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;use_nesterov&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/Momentum&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^lr/Momentum/update_lr/parameters/weights/ApplyMomentum&quot;\\n  input: &quot;^lr/Momentum/update_lr/parameters/bias/ApplyMomentum&quot;\\n}\\nnode {\\n  name: &quot;lr/init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^lr/parameters/weights/Assign&quot;\\n  input: &quot;^lr/parameters/bias/Assign&quot;\\n  input: &quot;^lr/lr/parameters/weights/Momentum/Assign&quot;\\n  input: &quot;^lr/lr/parameters/bias/Momentum/Assign&quot;\\n}\\nnode {\\n  name: &quot;lr/save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;W&quot;\\n        string_val: &quot;b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;lr/save/Const&quot;\\n  input: &quot;lr/save/SaveV2/tensor_names&quot;\\n  input: &quot;lr/save/SaveV2/shape_and_slices&quot;\\n  input: &quot;lr/parameters/weights&quot;\\n  input: &quot;lr/parameters/bias&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;lr/save/Const&quot;\\n  input: &quot;^lr/save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;lr/save/Const&quot;\\n  input: &quot;lr/save/RestoreV2/tensor_names&quot;\\n  input: &quot;lr/save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;lr/parameters/weights&quot;\\n  input: &quot;lr/save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/RestoreV2_1/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/RestoreV2_1/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/RestoreV2_1&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;lr/save/Const&quot;\\n  input: &quot;lr/save/RestoreV2_1/tensor_names&quot;\\n  input: &quot;lr/save/RestoreV2_1/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;lr/parameters/bias&quot;\\n  input: &quot;lr/save/RestoreV2_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@lr/parameters/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^lr/save/Assign&quot;\\n  input: &quot;^lr/save/Assign_1&quot;\\n}\\nnode {\\n  name: &quot;lr/cost/tags&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;lr/cost&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;lr/cost&quot;\\n  op: &quot;ScalarSummary&quot;\\n  input: &quot;lr/cost/tags&quot;\\n  input: &quot;lr/outputs/cost&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.84185493438&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open <jupyter-domain>:8000 in your browser to view the tensorboard graph\n",
    "os.system('tensorboard --logdir=\"%s\" --port=8000' % logdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
